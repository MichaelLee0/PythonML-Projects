{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_python = 'inline' in matplotlib.get_backend()\n",
    "if is_python: from IPython import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create DQN Class, Epsilon Greedy Strategy, Replay Memory and Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def _init_(self, img_height, img_width):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features = img_height * img_width * 3, out_features = 24)           # The \"*3\" corresponds to the three recieved color channels\n",
    "        self.fc2 = nn.Linear(in_features = 24, out_features = 32)\n",
    "        self.out = nn.Linear(in_features = 32, out_features = 2)            # Our only options here are either to move left or to move right (2d cart)\n",
    "\n",
    "    def forward(self,t):\n",
    "        t = t.flatten(start_dim=1)          # any initial input t must first be formatted. This gives us a ONE dimensional tensor (remember that's a matrix! and 0 dim is a vector)\n",
    "        t = F.relu(self.fc1(t))         # Pass it through the activation functions\n",
    "        t = F.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        return t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Experience = namedtuple(         # Create a tuple class named 'experience' (kinda like the self. objects of a class but without the methods that come with them)\n",
    "        'Experience',\n",
    "        ('state', 'action', 'next_state', 'reward')\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Experience(2,3,1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Experience(state=2, action=3, next_state=1, reward=4)"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.push_count = 0\n",
    "    \n",
    "    def push(self, experience):\n",
    "        if len(self.memory) < self.capacity:            # If the memory so far hasn't hit the capacity\n",
    "            self.memory.append(experience)\n",
    "        else:\n",
    "            self.memory[self.push % self.capacity] = experience     # The \"leftover\" (modulus) of push you're on by the total storage\n",
    "        self.push_count += 1\n",
    "\n",
    "    def sample(self, batch_size):           # grab batch_size number of samples from memory to train the network\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def can_provide_sample(self, batch_size):           # Do we have enough experiences to sample n_batchsize to train?\n",
    "        return len(self.memory) >= batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedyStrategy():\n",
    "    def __init__(self, start, end, decay):\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.decay = decay\n",
    "\n",
    "    def get_exploration_rate(self, current_step):           # Adds on top of the min (most likely to exploit), the range times an exponentially decreasing value\n",
    "        return self.end + (self.start - self.end) * \\           \n",
    "            math.exp(-1. * current_step * self.decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, strategy, num_actions):\n",
    "        self.current_step = 0           # Initializes at position 0\n",
    "        self.strategy = strategy            # This is our epsilon greedy number (threshhold)\n",
    "        self.num_actions = num_actions          # How many possible actions can an agent make from a given state\n",
    "\n",
    "    def select_action(self, state, policy_net):\n",
    "        rate = strategy.get_exploration_rate(self.current_step)         # Generate the random number that tells us either to explore or exploit\n",
    "        self.current_step += 1\n",
    "\n",
    "        if rate > random.random():\n",
    "            return random.randrange(self.num_actions)           # Give a random action from the ones available for that state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                return policy_net(state).argmax(dim= 1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "gamma = 0.999\n",
    "eps_start = 1\n",
    "eps_end = 0.01\n",
    "eps_decay = 0.001  \n",
    "target_update = 10          # Update the target network (as opposed to the policy network) every 10 moves\n",
    "memory_size = 100000\n",
    "lr = 0.001              # lr of the training of the policy network\n",
    "num_episodes = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "em = CartPoleEnvManager(device)         # I have no idea what this does\n",
    "strategy = EpsilonGreedyStrategy(eps_start, eps_end, eps_decay)         # Creates the strategy(ie threshhold)\n",
    "agent = Agent(strategy, em.num_actions_available(), device)         # Initializes our agen\n",
    "memory = ReplayMemory(memory_size)\n",
    "\n",
    "policy_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)          # The \"to\" means run it \"to\" our device\n",
    "target_net = DQN(em.get_screen_height(), em.get_screen_width()).to(device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitmachineengconda4c09815cb2e34abcb2c0919ed44690c2",
   "display_name": "Python 3.8.3 64-bit ('MachineEng': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}