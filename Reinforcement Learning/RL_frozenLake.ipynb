{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import gym \n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")     # We create an environment of Frozen lake game (special keyword) you can sample the environment, move the agent, collect rewards etc from this object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]\n [0. 0. 0. 0.]]\n"
    }
   ],
   "source": [
    "action_space_size = env.action_space.n      # Our action spaces are our column labels up top \n",
    "state_space_size = env.observation_space.n      # Our state space are our row labels\n",
    "\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10000         # How many \"rounds\" or episodes do you want the agent to play?\n",
    "max_steps_per_episode = 100         # How many steps will we allow the agent to play before we stop it (& reward 0 pts)\n",
    "\n",
    "learning_rate = 0.1         # our \"alpha\": How much should new observations contribute to overall learning?\n",
    "discount_rate = 0.99\n",
    "\n",
    "exploration_rate = 1         # Our \"epsilon\": where does the threshold for exploration/exploit initialize\n",
    "max_exploration_rate = 1\n",
    "min_exploration_rate = 0.01\n",
    "exploration_decay_rate = 0.0001           # How much do we make exploitation more probable over time? test against 0.01\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "*******Average Reward per thousand episodes******** \n\n1000 : 0.011000000000000003\n2000 : 0.011000000000000003\n3000 : 0.027000000000000017\n4000 : 0.027000000000000017\n5000 : 0.03800000000000003\n6000 : 0.058000000000000045\n7000 : 0.08400000000000006\n8000 : 0.08100000000000006\n9000 : 0.06700000000000005\n10000 : 0.10600000000000008\n\n\n ******* Q-Table ********\n\n[[0.58468075 0.54919178 0.54631778 0.5574945 ]\n [0.31269025 0.34736625 0.27196852 0.50609183]\n [0.44563606 0.43230446 0.44537003 0.47369003]\n [0.28026528 0.34774594 0.29096202 0.44334767]\n [0.60497032 0.31925645 0.41164321 0.29117521]\n [0.         0.         0.         0.        ]\n [0.33331677 0.30735129 0.28145161 0.15302066]\n [0.         0.         0.         0.        ]\n [0.33742983 0.48966997 0.37379552 0.63441513]\n [0.5226089  0.67380309 0.45911462 0.4692381 ]\n [0.64586059 0.52389945 0.31684041 0.32235661]\n [0.         0.         0.         0.        ]\n [0.         0.         0.         0.        ]\n [0.48112697 0.65650214 0.81456123 0.50424778]\n [0.72102932 0.92979337 0.80320062 0.74462125]\n [0.         0.         0.         0.        ]]\n"
    }
   ],
   "source": [
    "rewards_all_episodes = []       # Holds the rewards we will get from each episode (so we can view our our game scores change over time)\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False            # Resets \"done\" from the previous episode\n",
    "    rewards_current_episode = 0         # what were the total rewards for this past episode?\n",
    "\n",
    "    for step in range(max_steps_per_episode):           #iterates through our maximum allowed steps\n",
    "\n",
    "        # Exploration vs exploitation?\n",
    "        exploration_rate_threshold = random.uniform(0,1)            # Generates a random number tells us if it will explore or exploit\n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])            # if the random number is larger than our threshold sets the action to the one that promises the largest expected return\n",
    "\n",
    "        else:\n",
    "            action = env.action_space.sample()          # sets our action to a random one (i.e. sample state space)\n",
    "        \n",
    "        new_state, reward, done, infor = env.step(action)           # we take the action as determined by action\n",
    "\n",
    "        # Update Q-table for A(s,a)\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + discount_rate * np.max(q_table[new_state,:]))            # So we use the past state and newly decided action into the bellman equation. Notice that the \"max\" term takes the max possible return only from the actions right after this one. It can therefore be applies recursively.\n",
    "\n",
    "        state = new_state           # Update the state \n",
    "        rewards_current_episode += reward           # Store away the rewards for this step sequentially in the log of all rewards for the episode\n",
    "\n",
    "        if done == True:            # If you're done, jump up and restart the episode (or end it if you got to the end of num_of_episodes) else, continue making steps\n",
    "            break\n",
    "        \n",
    "    \n",
    "    # Exploration rate decay\n",
    "    exploration_rate = min_exploration_rate +(max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate * episode)       # multiply our decay factor by a larger number as episodes increase\n",
    "\n",
    "    rewards_all_episodes.append(rewards_current_episode)\n",
    "\n",
    "rewards_per_thousand_episodes = np.split(np.array(rewards_all_episodes), num_episodes/1000)     # Seperates out our rewards roughly into groups representing the rewards from 1000 episodes\n",
    "count = 1000\n",
    "print(\"*******Average Reward per thousand episodes******** \\n\")\n",
    "for r in rewards_per_thousand_episodes:\n",
    "    print(count, \":\", str(sum(r/1000)))\n",
    "    count += 1000               # Adds up the average of the next 1000 episodes\n",
    "\n",
    "# Print updates Q-table\n",
    "print(\"\\n\\n ******* Q-Table ********\\n\")\n",
    "print(q_table)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(Down)\nSFFF\nFHFH\nFFFH\nHFF\u001b[41mG\u001b[0m\n****You reached the goal!****\n"
    }
   ],
   "source": [
    "# Visualize the robot playing the game\n",
    "\n",
    "for episode in range(3):         # Watch 3 episodes\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    print(\"***** Episode\", episode + 1, \"*****\\n\\n\\n\\n\")\n",
    "    time.sleep(1)           # Just so that we can read the title\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        clear_output(wait = True)           # clear_output is an ipython command that clears the output of the cell and wait = True means it waits to clear the display until it gets another output\n",
    "        env.render()            # Render the state of our agent\n",
    "        time.sleep(0.3)\n",
    "\n",
    "        action = np.argmax(q_table[state,:])            # Set our action to be the one that maximizes our expected return\n",
    "        new_state, reward, done, info = env.step(action)    \n",
    "\n",
    "        if done:\n",
    "            clear_output(wait = True)\n",
    "            env.render()\n",
    "            if reward == 1:\n",
    "                print(\"****You reached the goal!****\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"****You fell through a hole!****\")\n",
    "                time.sleep(0.3)\n",
    "\n",
    "            clear_output(wait = True)\n",
    "            break\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitmachineengconda4c09815cb2e34abcb2c0919ed44690c2",
   "display_name": "Python 3.8.3 64-bit ('MachineEng': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}